{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PYSOLR_PATH = '/home/ubuntu/code/NFGEC/'\n",
    "import sys, os\n",
    "if not PYSOLR_PATH in sys.path:\n",
    "    sys.path.append(PYSOLR_PATH)\n",
    "    \n",
    "os.chdir(PYSOLR_PATH)\n",
    "\n",
    "import argparse\n",
    "from sklearn.externals import joblib\n",
    "from src.model.nn_model import Model\n",
    "from src.batcher import Batcher\n",
    "from src.hook import acc_hook, save_predictions\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"dataset\",help=\"dataset to train model\",choices=[\"figer\",\"gillick\"])\n",
    "# parser.add_argument(\"encoder\",help=\"context encoder to use in model\",choices=[\"averaging\",\"lstm\",\"attentive\"])\n",
    "# parser.add_argument('--feature', dest='feature', action='store_true')\n",
    "# parser.add_argument('--no-feature', dest='feature', action='store_false')\n",
    "# parser.set_defaults(feature=False)\n",
    "# parser.add_argument('--hier', dest='hier', action='store_true')\n",
    "# parser.add_argument('--no-hier', dest='hier', action='store_false')\n",
    "# parser.set_defaults(hier=False)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "print \"Creating the model\"\n",
    "model = Model(type='figer',encoder='attentive',hier=True,feature=True)\n",
    "\n",
    "print \"Loading the dictionaries\"\n",
    "d = \"Wiki\" \n",
    "dicts = joblib.load(\"data/\"+d+\"/dicts_\"+\"figer\"+\".pkl\")\n",
    "\n",
    "print \"Loading the datasets\"\n",
    "train_dataset = joblib.load(\"data/\"+d+\"/train_\"+\"figer\"+\".pkl\")\n",
    "dev_dataset = joblib.load(\"data/\"+d+\"/dev_\"+\"figer\"+\".pkl\")\n",
    "test_dataset = joblib.load(\"data/\"+d+\"/test_\"+\"figer\"+\".pkl\")\n",
    "\n",
    "print \n",
    "print \"train_size:\", train_dataset[\"data\"].shape[0]\n",
    "print \"dev_size: \", dev_dataset[\"data\"].shape[0]\n",
    "print \"test_size: \", test_dataset[\"data\"].shape[0]\n",
    "\n",
    "print \"Creating batchers\"\n",
    "# batch_size : 1000, context_length : 10\n",
    "train_batcher = Batcher(train_dataset[\"storage\"],train_dataset[\"data\"],1000,10,dicts[\"id2vec\"])\n",
    "dev_batcher = Batcher(dev_dataset[\"storage\"],dev_dataset[\"data\"],dev_dataset[\"data\"].shape[0],10,dicts[\"id2vec\"])\n",
    "test_batcher = Batcher(test_dataset[\"storage\"],test_dataset[\"data\"],test_dataset[\"data\"].shape[0],10,dicts[\"id2vec\"])\n",
    "\n",
    "step_par_epoch = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start trainning\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/model/nn_model.py:135: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if self.feature == True and feature_data != None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------dev--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/model/nn_model.py:154: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if self.feature == True and feature_data != None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('     strict (p,r,f1):', (0.5703, 0.5703, 0.5703))\n",
      "('loose macro (p,r,f1):', (0.8760277380952389, 0.7811042857142863, 0.8258473203091451))\n",
      "('loose micro (p,r,f1):', (0.8945049504950495, 0.7624050632911392, 0.8231890660592255))\n",
      "epoch 1\n",
      "------dev--------\n",
      "('     strict (p,r,f1):', (0.6487, 0.6487, 0.6487))\n",
      "('loose macro (p,r,f1):', (0.8907764285714292, 0.8334844047619062, 0.8611785954779195))\n",
      "('loose micro (p,r,f1):', (0.9048568910330751, 0.8230379746835443, 0.8620102967496742))\n",
      "epoch 2\n",
      "------dev--------\n",
      "('     strict (p,r,f1):', (0.6889, 0.6889, 0.6889))\n",
      "('loose macro (p,r,f1):', (0.8937751190476196, 0.8652646428571436, 0.8792888323796855))\n",
      "('loose micro (p,r,f1):', (0.9030190514078592, 0.8619831223628692, 0.8820240485288087))\n",
      "epoch 3\n",
      "------dev--------\n",
      "('     strict (p,r,f1):', (0.7138, 0.7138, 0.7138))\n",
      "('loose macro (p,r,f1):', (0.9047506746031744, 0.8734916666666681, 0.8888464258618111))\n",
      "('loose micro (p,r,f1):', (0.9143287683375437, 0.8704641350210971, 0.8918574238592396))\n",
      "epoch 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"start trainning\"\n",
    "for epoch in range(5):\n",
    "    train_batcher.shuffle()\n",
    "    print \"epoch\",epoch\n",
    "    for i in range(step_par_epoch):\n",
    "        context_data, mention_representation_data, target_data, feature_data = train_batcher.next()\n",
    "        model.train(context_data, mention_representation_data, target_data, feature_data)\n",
    "        \n",
    "    print \"------dev--------\"\n",
    "    context_data, mention_representation_data, target_data, feature_data = dev_batcher.next()\n",
    "    scores = model.predict(context_data, mention_representation_data,feature_data)\n",
    "    acc_hook(scores, target_data)\n",
    "\n",
    "print \"Training completed.  Below are the final test scores: \"\n",
    "print \"-----test--------\"\n",
    "context_data, mention_representation_data, target_data, feature_data = test_batcher.next()\n",
    "scores = model.predict(context_data, mention_representation_data, feature_data)\n",
    "acc_hook(scores, target_data)\n",
    "fname = \"figer\" + \"_\" + \"attentive\" + \"_\" + \"true\" + \"_\" + \"true\" + \".txt\"\n",
    "save_predictions(scores, target_data, dicts[\"id2label\"],fname)\n",
    "\n",
    "print \"Cheers!\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----test--------\n",
      "('     strict (p,r,f1):', (0.5914742451154529, 0.5914742451154529, 0.5914742451154529))\n",
      "('loose macro (p,r,f1):', (0.7762771716146492, 0.818679692125518, 0.7969147884930902))\n",
      "('loose micro (p,r,f1):', (0.717741935483871, 0.8038709677419354, 0.758368837492392))\n",
      "Cheers!\n"
     ]
    }
   ],
   "source": [
    "print \"-----test--------\"\n",
    "context_data, mention_representation_data, target_data, feature_data = test_batcher.next()\n",
    "scores = model.predict(context_data, mention_representation_data, feature_data)\n",
    "acc_hook(scores, target_data)\n",
    "fname = \"figer\" + \"_\" + \"attentive\" + \"_\" + \"true\" + \"_\" + \"true\" + \".txt\"\n",
    "save_predictions(scores, target_data, dicts[\"id2label\"],fname)\n",
    "\n",
    "print \"Cheers!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dataset = joblib.load(\"data/Wiki/new_test_figer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dataset_batcher = Batcher(new_dataset[\"storage\"],new_dataset[\"data\"],new_dataset[\"data\"].shape[0],10,dicts[\"id2vec\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----test--------\n",
      "('     strict (p,r,f1):', (0.3449775902979172, 0.3449775902979172, 0.3449775902979172))\n",
      "('loose macro (p,r,f1):', (0.585438976979315, 0.6773662008963881, 0.6280566197237739))\n",
      "('loose micro (p,r,f1):', (0.55805270338192, 0.7034757422157857, 0.6223823823823824))\n",
      "Cheers!\n"
     ]
    }
   ],
   "source": [
    "print \"-----test--------\"\n",
    "context_data, mention_representation_data, target_data, feature_data = new_dataset_batcher.next()\n",
    "scores = model.predict(context_data, mention_representation_data, feature_data)\n",
    "acc_hook(scores, target_data)\n",
    "fname = \"new_dataset_\" + \"_\" + \"attentive\" + \"_\" + \"true\" + \"_\" + \"true\" + \".txt\"\n",
    "save_predictions(scores, target_data, dicts[\"id2label\"],fname)\n",
    "\n",
    "print \"Cheers!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scores.plk']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scores, \"scores.plk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '/location',\n",
       " 1: '/person',\n",
       " 2: '/organization',\n",
       " 3: '/location/city',\n",
       " 4: '/person/artist',\n",
       " 5: '/location/country',\n",
       " 6: '/person/author',\n",
       " 7: '/person/actor',\n",
       " 8: '/organization/company',\n",
       " 9: '/event',\n",
       " 10: '/government',\n",
       " 11: '/organization/sports_team',\n",
       " 12: '/government/government',\n",
       " 13: '/person/athlete',\n",
       " 14: '/title',\n",
       " 15: '/location/cemetery',\n",
       " 16: '/person/musician',\n",
       " 17: '/location/province',\n",
       " 18: '/building',\n",
       " 19: '/person/politician',\n",
       " 20: '/organization/educational_institution',\n",
       " 21: '/language',\n",
       " 22: '/people/ethnicity',\n",
       " 23: '/people',\n",
       " 24: '/government_agency',\n",
       " 25: '/written_work',\n",
       " 26: '/location/county',\n",
       " 27: '/person/director',\n",
       " 28: '/living_thing',\n",
       " 29: '/art',\n",
       " 30: '/geography',\n",
       " 31: '/product',\n",
       " 32: '/art/film',\n",
       " 33: '/event/military_conflict',\n",
       " 34: '/military',\n",
       " 35: '/broadcast_program',\n",
       " 36: '/music',\n",
       " 37: '/location/body_of_water',\n",
       " 38: '/organization/sports_league',\n",
       " 39: '/geography/island',\n",
       " 40: '/news_agency',\n",
       " 41: '/government/political_party',\n",
       " 42: '/transportation/road',\n",
       " 43: '/transportation',\n",
       " 44: '/person/engineer',\n",
       " 45: '/park',\n",
       " 46: '/religion',\n",
       " 47: '/religion/religion',\n",
       " 48: '/person/monarch',\n",
       " 49: '/broadcast_network',\n",
       " 50: '/software',\n",
       " 51: '/event/attack',\n",
       " 52: '/education',\n",
       " 53: '/education/educational_degree',\n",
       " 54: '/person/soldier',\n",
       " 55: '/event/sports_event',\n",
       " 56: '/geography/mountain',\n",
       " 57: '/medicine',\n",
       " 58: '/disease',\n",
       " 59: '/event/election',\n",
       " 60: '/food',\n",
       " 61: '/play',\n",
       " 62: '/game',\n",
       " 63: '/product/ship',\n",
       " 64: '/chemistry',\n",
       " 65: '/building/sports_facility',\n",
       " 66: '/person/coach',\n",
       " 67: '/product/airplane',\n",
       " 68: '/award',\n",
       " 69: '/god',\n",
       " 70: '/livingthing/animal',\n",
       " 71: '/livingthing',\n",
       " 72: '/law',\n",
       " 73: '/medicine/medical_treatment',\n",
       " 74: '/internet/website',\n",
       " 75: '/internet',\n",
       " 76: '/time',\n",
       " 77: '/person/religious_leader',\n",
       " 78: '/astral_body',\n",
       " 79: '/building/airport',\n",
       " 80: '/event/natural_disaster',\n",
       " 81: '/product/car',\n",
       " 82: '/person/architect',\n",
       " 83: '/product/instrument',\n",
       " 84: '/product/computer',\n",
       " 85: '/medicine/symptom',\n",
       " 86: '/computer',\n",
       " 87: '/organization/airline',\n",
       " 88: '/computer/programming_language',\n",
       " 89: '/building/theater',\n",
       " 90: '/medicine/drug',\n",
       " 91: '/person/doctor',\n",
       " 92: '/body_part',\n",
       " 93: '/finance',\n",
       " 94: '/product/weapon',\n",
       " 95: '/transit',\n",
       " 96: '/event/protest',\n",
       " 97: '/metropolitan_transit/transit_line',\n",
       " 98: '/metropolitan_transit',\n",
       " 99: '/finance/currency',\n",
       " 100: '/rail',\n",
       " 101: '/rail/railway',\n",
       " 102: '/building/hospital',\n",
       " 103: '/location/bridge',\n",
       " 104: '/building/restaurant',\n",
       " 105: '/broadcast/tv_channel',\n",
       " 106: '/broadcast',\n",
       " 107: '/product/spacecraft',\n",
       " 108: '/biology',\n",
       " 109: '/train',\n",
       " 110: '/organization/fraternity_sorority',\n",
       " 111: '/education/department',\n",
       " 112: '/building/library'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts[\"id2label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/Wiki/dicts_figer.pkl'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"data/\"+d+\"/dicts_\"+\"figer\"+\".pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_true_and_prediction(scores, y_data):\n",
    "    true_and_prediction = []\n",
    "    for score,true_label in zip(scores,y_data):\n",
    "        predicted_tag = []\n",
    "        true_tag = []\n",
    "        for label_id,label_score in enumerate(list(true_label)):\n",
    "            if label_score > 0:\n",
    "                true_tag.append(label_id)\n",
    "        lid,ls = max(enumerate(list(score)),key=lambda x: x[1])\n",
    "        predicted_tag.append(lid)\n",
    "        for label_id,label_score in enumerate(list(score)):\n",
    "            if label_score > 0.5:\n",
    "                if label_id != lid:\n",
    "                    predicted_tag.append((label_id, label_score))\n",
    "        true_and_prediction.append((true_tag, predicted_tag))\n",
    "    return true_and_prediction\n",
    "\n",
    "def acc_hook(scores, y_data):\n",
    "    true_and_prediction = get_true_and_prediction(scores, y_data)\n",
    "    print(\"     strict (p,r,f1):\",strict(true_and_prediction))\n",
    "    print(\"loose macro (p,r,f1):\",loose_macro(true_and_prediction))\n",
    "    print(\"loose micro (p,r,f1):\",loose_micro(true_and_prediction))\n",
    "\n",
    "def save_predictions(scores, y_data, id2label, fname):\n",
    "    true_and_prediction = get_true_and_prediction(scores, y_data)\n",
    "    with open(fname,\"w\") as f:\n",
    "        for t, p in true_and_prediction:\n",
    "            f.write(\" \".join([id2label[id] for id in t]) + \"\\t\" + \" \".join([id2label[id] for id in p]) + \"\\n\")\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucl-fine",
   "language": "python",
   "name": "ucl-fine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
